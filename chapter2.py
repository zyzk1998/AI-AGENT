import time
import sqlite3
import asyncio
import aiohttp
from functools import wraps
from typing import Dict, List, Callable, Any

# å…¨å±€é…ç½®ï¼šæœ¬åœ°OllamaæœåŠ¡åœ°å€ï¼ˆä¸Dockerç«¯å£æ˜ å°„ä¸€è‡´ï¼‰
OLLAMA_BASE_URL = "http://localhost:11435"
# é€‰æ‹©å®¹å™¨å†…å·²æœ‰çš„æ¨¡å‹ï¼ˆå¿…é¡»ä¸ `ollama list` æ˜¾ç¤ºä¸€è‡´ï¼‰
SELECTED_MODEL = "llama3:latest"
# å¤©æ°”API Keyï¼ˆéœ€æ›¿æ¢ä¸ºä½ çš„çœŸå®Keyï¼Œå¦åˆ™å¤©æ°”æŸ¥è¯¢åŠŸèƒ½ä¸å¯ç”¨ï¼‰
WEATHER_API_KEY = "your_weather_api_key_here"

# ------------------------------
# 1. è®°å¿†ç®¡ç†æ¨¡å—ï¼ˆå­˜å‚¨ç”¨æˆ·æŸ¥è¯¢å†å²ï¼‰
# ------------------------------
class MemoryManager:
    """ç®¡ç†ç”¨æˆ·æŸ¥è¯¢å†å²çš„æ•°æ®åº“æ¨¡å—"""
    def __init__(self, db_path: str = "customer_history.db"):
        self.db_path = db_path
        self._init_db()

    def _init_db(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS customer_queries (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                customer_id TEXT NOT NULL,
                query TEXT NOT NULL,
                timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)
        conn.commit()
        conn.close()

    def save_query(self, customer_id: str, query: str):
        """ä¿å­˜ç”¨æˆ·æŸ¥è¯¢åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            INSERT INTO customer_queries (customer_id, query)
            VALUES (?, ?)
        """, (customer_id, query))
        conn.commit()
        conn.close()

    def get_last_query(self, customer_id: str) -> str:
        """è·å–ç”¨æˆ·æœ€åä¸€æ¬¡æŸ¥è¯¢"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("""
            SELECT query FROM customer_queries
            WHERE customer_id = ?
            ORDER BY timestamp DESC
            LIMIT 1
        """, (customer_id,))
        result = cursor.fetchone()
        conn.close()
        return result[0] if result else "æœªæŸ¥è¯¢åˆ°å†å²è®°å½•"

# åˆå§‹åŒ–è®°å¿†ç®¡ç†å™¨ï¼ˆå…¨å±€å•ä¾‹ï¼‰
memory_manager = MemoryManager()

# ------------------------------
# 2. æ€§èƒ½ç›‘æ§è£…é¥°å™¨ï¼ˆç»Ÿè®¡å‡½æ•°è¿è¡Œæ—¶é—´ï¼‰
# ------------------------------
def performance_monitor(func: Callable) -> Callable:
    @wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        result = await func(*args, **kwargs)
        elapsed = round(time.time() - start_time, 2)
        print(f"\nâœ… ã€{func.__name__}ã€‘è¿è¡Œå®Œæˆï¼Œè€—æ—¶ {elapsed} ç§’")
        return result
    return wrapper

# ------------------------------
# 3. OllamaæœåŠ¡è°ƒç”¨æ¨¡å—ï¼ˆæ ¸å¿ƒï¼šè¿æ¥æœ¬åœ°Dockerå†…çš„LLMï¼‰
# ------------------------------
class OllamaManager:
    """ç®¡ç†ä¸æœ¬åœ°Ollama DockeræœåŠ¡çš„äº¤äº’"""
    def __init__(self, model_name: str = SELECTED_MODEL, base_url: str = OLLAMA_BASE_URL):
        self.model_name = model_name
        self.base_url = base_url
        self.session = None  # å¼‚æ­¥ä¼šè¯å»¶è¿Ÿåˆå§‹åŒ–

    async def __aenter__(self):
        """é€šè¿‡ async with è‡ªåŠ¨åˆ›å»ºä¼šè¯"""
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """é€šè¿‡ async with è‡ªåŠ¨å…³é—­ä¼šè¯"""
        await self.close()

    async def close(self):
        """å…³é—­å¼‚æ­¥ä¼šè¯"""
        if self.session and not self.session.closed:
            await self.session.close()

    async def generate_text(self, prompt: str, max_tokens: int = 500) -> str:
        """
        è°ƒç”¨Ollamaç”Ÿæˆä¸­æ–‡å›ç­”
        :param prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯
        :param max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°ï¼ˆæ§åˆ¶å›ç­”é•¿åº¦ï¼‰
        :return: ä¸­æ–‡å›ç­”æ–‡æœ¬
        """
        if not self.session:
            return "âŒ Ollamaä¼šè¯æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå›ç­”"

        # å¼ºåˆ¶æ·»åŠ ä¸­æ–‡æŒ‡ä»¤ï¼Œç¡®ä¿è¾“å‡ºä¸ºä¸­æ–‡
        chinese_prompt = f"""
        è¯·ç”¨ç®€æ´ã€ä¸“ä¸šçš„ä¸­æ–‡å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œé¿å…ä½¿ç”¨è‹±æ–‡ã€‚
        è‹¥é—®é¢˜æ¶‰åŠæŠ•èµ„å»ºè®®ï¼Œéœ€åŸºäºä¿å®ˆã€ç¨³å¥çš„åŸåˆ™ï¼›è‹¥æ¶‰åŠæ¦‚å¿µè§£é‡Šï¼Œéœ€é€šä¿—æ˜“æ‡‚ã€‚
        
        é—®é¢˜ï¼š{prompt}
        """

        try:
            response = await self.session.post(
                url=f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": chinese_prompt,
                    "max_tokens": max_tokens,
                    "stream": False,  # å…³é—­æµå¼è¾“å‡ºï¼Œé€‚åˆæ‰¹é‡å¤„ç†
                    "temperature": 0.7  # æ§åˆ¶éšæœºæ€§ï¼ˆ0.7ä¸ºå¹³è¡¡å€¼ï¼‰
                }
            )
            response.raise_for_status()  # è§¦å‘HTTPé”™è¯¯ï¼ˆå¦‚404ã€500ï¼‰
            data = await response.json()
            return data.get("response", "âŒ æœªè·å–åˆ°æ¨¡å‹å›ç­”ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡")

        except aiohttp.ClientError as e:
            return f"âŒ è°ƒç”¨Ollamaå¤±è´¥ï¼šç½‘ç»œé”™è¯¯ï¼ˆ{str(e)}ï¼‰"
        except Exception as e:
            return f"âŒ è°ƒç”¨Ollamaå¼‚å¸¸ï¼š{str(e)}"

# ------------------------------
# 4. LLMè¯­ä¹‰æ£€ç´¢æ¨¡å—ï¼ˆæ›¿ä»£åŸsentence-transformersï¼‰
# ------------------------------
class LLMSemanticSearch:
    """ä½¿ç”¨LLMåˆ¤æ–­è¯­ä¹‰ç›¸å…³æ€§ï¼Œå®ç°çŸ¥è¯†åº“æ£€ç´¢"""
    def __init__(self, ollama_manager: OllamaManager):
        self.ollama_manager = ollama_manager
        # é‡‘èçŸ¥è¯†åº“ï¼ˆå¯æ ¹æ®éœ€æ±‚æ‰©å±•ï¼‰
        self.knowledge_base = [
            "è‚¡ç¥¨å¸‚åœºæ˜¯é£é™©è¾ƒé«˜çš„æŠ•èµ„æ¸ é“ï¼Œé€‚åˆèƒ½æ‰¿å—çŸ­æœŸæ³¢åŠ¨çš„æŠ•èµ„è€…ã€‚",
            "å€ºåˆ¸æŠ•èµ„é£é™©è¾ƒä½ã€æ”¶ç›Šç¨³å®šï¼Œé€‚åˆä¿å®ˆå‹æŠ•èµ„è€…ã€‚",
            "å¤–æ±‡å¸‚åœºæ³¢åŠ¨å‰§çƒˆï¼Œå¯¹ä¸“ä¸šçŸ¥è¯†è¦æ±‚é«˜ï¼Œä¸é€‚åˆæ–°æ‰‹ã€‚",
            "è´§å¸åŸºé‡‘æµåŠ¨æ€§å¼ºã€é£é™©æä½ï¼Œé€‚åˆå­˜æ”¾çŸ­æœŸå¤‡ç”¨èµ„é‡‘ã€‚",
            "æŒ‡æ•°åŸºé‡‘é€šè¿‡è·Ÿè¸ªå¤§ç›˜åˆ†æ•£é£é™©ï¼Œé€‚åˆé•¿æœŸå®šæŠ•ã€‚"
        ]

    async def _score_relevance(self, query: str, knowledge: str) -> float:
        """
        è®©LLMç»™â€œæŸ¥è¯¢-çŸ¥è¯†åº“æ¡ç›®â€çš„ç›¸å…³æ€§æ‰“åˆ†ï¼ˆ0-10åˆ†ï¼‰
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :param knowledge: çŸ¥è¯†åº“æ¡ç›®
        :return: ç›¸å…³æ€§åˆ†æ•°ï¼ˆ0=å®Œå…¨ä¸ç›¸å…³ï¼Œ10=é«˜åº¦ç›¸å…³ï¼‰
        """
        score_prompt = f"""
        è¯·ä»…è¿”å›ä¸€ä¸ª0-10çš„æ•°å­—ï¼Œç”¨äºè¡¨ç¤ºâ€œç”¨æˆ·æŸ¥è¯¢â€ä¸â€œçŸ¥è¯†åº“æ¡ç›®â€çš„è¯­ä¹‰ç›¸å…³æ€§ï¼š
        - 0åˆ†ï¼šå®Œå…¨ä¸ç›¸å…³ï¼ˆå¦‚æŸ¥è¯¢å¤©æ°” vs æŠ•èµ„çŸ¥è¯†ï¼‰
        - 5åˆ†ï¼šéƒ¨åˆ†ç›¸å…³ï¼ˆå¦‚æŸ¥è¯¢â€œçŸ­æœŸç†è´¢â€ vs â€œè´§å¸åŸºé‡‘â€ï¼‰
        - 10åˆ†ï¼šé«˜åº¦ç›¸å…³ï¼ˆå¦‚æŸ¥è¯¢â€œä¿å®ˆæŠ•èµ„â€ vs â€œå€ºåˆ¸æŠ•èµ„â€ï¼‰
        
        ç”¨æˆ·æŸ¥è¯¢ï¼š{query}
        çŸ¥è¯†åº“æ¡ç›®ï¼š{knowledge}
        ç›¸å…³æ€§åˆ†æ•°ï¼š
        """

        score_str = await self.ollama_manager.generate_text(score_prompt, max_tokens=10)
        try:
            # æå–æ•°å­—ï¼ˆå¤„ç†å¯èƒ½çš„å¤šä½™å­—ç¬¦ï¼Œå¦‚â€œåˆ†æ•°ï¼š8â€â†’8ï¼‰
            score = float([c for c in score_str if c.isdigit() or c == "."][0])
            return max(0.0, min(10.0, score))  # é™åˆ¶åˆ†æ•°åœ¨0-10ä¹‹é—´
        except:
            return 3.0  # è§£æå¤±è´¥æ—¶è¿”å›é»˜è®¤åˆ†æ•°

    async def search(self, query: str) -> str:
        """
        æ£€ç´¢çŸ¥è¯†åº“ä¸­ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„æ¡ç›®
        :param query: ç”¨æˆ·æŸ¥è¯¢
        :return: æœ€ç›¸å…³çš„çŸ¥è¯†åº“æ¡ç›®
        """
        if not self.knowledge_base:
            return "âŒ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•æ£€ç´¢"

        print("\nğŸ” æ­£åœ¨è¿›è¡ŒçŸ¥è¯†åº“è¯­ä¹‰æ£€ç´¢...")
        # æ‰¹é‡è®¡ç®—æ¯ä¸ªæ¡ç›®çš„ç›¸å…³æ€§åˆ†æ•°
        relevance_scores = []
        for idx, knowledge in enumerate(self.knowledge_base, 1):
            score = await self._score_relevance(query, knowledge)
            relevance_scores.append((score, knowledge))
            print(f"   æ¡ç›®{idx}ï¼š{knowledge[:20]}... ç›¸å…³æ€§åˆ†æ•°ï¼š{score:.1f}")

        # è¿”å›åˆ†æ•°æœ€é«˜çš„æ¡ç›®
        best_match = max(relevance_scores, key=lambda x: x[0])[1]
        print(f"âœ… æ£€ç´¢å®Œæˆï¼Œæœ€ç›¸å…³æ¡ç›®ï¼š{best_match}")
        return best_match

# ------------------------------
# 5. å¤–éƒ¨APIè°ƒç”¨æ¨¡å—ï¼ˆå¤©æ°”æŸ¥è¯¢ï¼‰
# ------------------------------
class APIManager:
    """ç®¡ç†å¤–éƒ¨APIè°ƒç”¨ï¼ˆå¦‚å¤©æ°”æŸ¥è¯¢ï¼‰"""
    @staticmethod
    async def get_weather(city: str) -> str:
        """
        è°ƒç”¨å¤©æ°”APIè·å–åŸå¸‚æ¸©åº¦ï¼ˆéœ€æ›¿æ¢WEATHER_API_KEYï¼‰
        :param city: åŸå¸‚åç§°ï¼ˆå¦‚â€œåŒ—äº¬â€â€œä¸Šæµ·â€ï¼‰
        :return: å¤©æ°”ä¿¡æ¯
        """
        if WEATHER_API_KEY == "your_weather_api_key_here":
            return "âŒ å¤©æ°”æŸ¥è¯¢åŠŸèƒ½æœªå¯ç”¨ï¼šè¯·æ›¿æ¢ä»£ç ä¸­çš„ WEATHER_API_KEYï¼ˆå¯ä»weatherapi.comè·å–å…è´¹Keyï¼‰"

        try:
            async with aiohttp.ClientSession() as session:
                response = await session.get(
                    url=f"http://api.weatherapi.com/v1/current.json",
                    params={
                        "key": WEATHER_API_KEY,
                        "q": city,
                        "aqi": "no"  # ä¸è¿”å›ç©ºæ°”è´¨é‡æ•°æ®
                    }
                )
                response.raise_for_status()
                data = await response.json()
                temp_c = data["current"]["temp_c"]
                condition = data["current"]["condition"]["text"]
                return f"ğŸŒ¤ï¸ {city}å½“å‰å¤©æ°”ï¼š{condition}ï¼Œæ°”æ¸© {temp_c}â„ƒ"
        except Exception as e:
            return f"âŒ å¤©æ°”æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"

# ------------------------------
# 6. æ ¸å¿ƒæ™ºèƒ½ä½“æ¨¡å—ï¼ˆæ•´åˆæ‰€æœ‰åŠŸèƒ½ï¼‰
# ------------------------------
class FinancialAgent:
    """é‡‘èæ™ºèƒ½ä½“ï¼šæ•´åˆè®°å¿†ã€LLMã€æ£€ç´¢ã€APIåŠŸèƒ½"""
    def __init__(self, ollama_manager: OllamaManager, semantic_search: LLMSemanticSearch):
        self.ollama_manager = ollama_manager
        self.semantic_search = semantic_search

    @performance_monitor
    async def handle_user_query(self, customer_id: str, query: str) -> None:
        """
        å¤„ç†ç”¨æˆ·æŸ¥è¯¢çš„ä¸»å…¥å£
        :param customer_id: ç”¨æˆ·IDï¼ˆç”¨äºè®°å¿†è·Ÿè¸ªï¼‰
        :param query: ç”¨æˆ·è¾“å…¥çš„æŸ¥è¯¢æ–‡æœ¬
        """
        # 1. ä¿å­˜æŸ¥è¯¢å†å²
        memory_manager.save_query(customer_id, query)
        print(f"\nğŸ“Œ æ­£åœ¨å¤„ç†ç”¨æˆ·ã€Œ{customer_id}ã€çš„æŸ¥è¯¢ï¼š{query}")

        # 2. åˆ†æ”¯1ï¼šå¤©æ°”æŸ¥è¯¢
        if any(keyword in query for keyword in ["å¤©æ°”", "æ°”æ¸©", "æ¸©åº¦"]):
            # æå–åŸå¸‚åï¼ˆç®€å•è§„åˆ™ï¼šå–æŸ¥è¯¢æœ€å2-3ä¸ªæ±‰å­—ï¼Œå¦‚â€œåŒ—äº¬å¤©æ°”â€â†’â€œåŒ—äº¬â€ï¼‰
            city = "".join([c for c in query if '\u4e00' <= c <= '\u9fff'])[-3:] or "åŒ—äº¬"
            weather_info = await APIManager.get_weather(city)
            print(f"\nğŸ“Š å¤©æ°”æŸ¥è¯¢ç»“æœï¼š{weather_info}")

        # 3. åˆ†æ”¯2ï¼šæŠ•èµ„ç›¸å…³æŸ¥è¯¢ï¼ˆè§¦å‘çŸ¥è¯†åº“æ£€ç´¢ï¼‰
        elif any(keyword in query for keyword in ["æŠ•èµ„", "ç†è´¢", "åŸºé‡‘", "å€ºåˆ¸", "è‚¡ç¥¨"]):
            # æ£€ç´¢çŸ¥è¯†åº“æœ€ç›¸å…³æ¡ç›®
            relevant_knowledge = await self.semantic_search.search(query)
            # åŸºäºçŸ¥è¯†åº“ç”Ÿæˆå¢å¼ºå›ç­”
            final_answer = await self.ollama_manager.generate_text(
                prompt=f"åŸºäºä»¥ä¸‹çŸ¥è¯†åº“ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š\nçŸ¥è¯†åº“ï¼š{relevant_knowledge}\nç”¨æˆ·é—®é¢˜ï¼š{query}"
            )
            print(f"\nğŸ“Š æŠ•èµ„å»ºè®®ç»“æœï¼š\n{final_answer}")

        # 4. åˆ†æ”¯3ï¼šé€šç”¨é—®é¢˜ï¼ˆç›´æ¥è°ƒç”¨LLMï¼‰
        else:
            general_answer = await self.ollama_manager.generate_text(prompt=query)
            print(f"\nğŸ“Š é€šç”¨é—®é¢˜å›ç­”ï¼š\n{general_answer}")

# ------------------------------
# 7. ä¸»ç¨‹åºå…¥å£
# ------------------------------
async def main():
    print(f"=====================================")
    print(f"ğŸš€ é‡‘èæ™ºèƒ½ä½“å¯åŠ¨ï¼ˆä½¿ç”¨æ¨¡å‹ï¼š{SELECTED_MODEL}ï¼‰")
    print(f"ğŸ“Œ OllamaæœåŠ¡åœ°å€ï¼š{OLLAMA_BASE_URL}")
    print(f"=====================================")

    # åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶ï¼ˆé€šè¿‡async withè‡ªåŠ¨ç®¡ç†ä¼šè¯ç”Ÿå‘½å‘¨æœŸï¼‰
    async with OllamaManager() as ollama_manager:
        semantic_search = LLMSemanticSearch(ollama_manager)
        agent = FinancialAgent(ollama_manager, semantic_search)

        # æµ‹è¯•ç”¨ä¾‹ï¼ˆå¯æ›¿æ¢ä¸ºå®é™…ç”¨æˆ·æŸ¥è¯¢ï¼‰
        test_cases = [
            ("customer_001", "æˆ‘æ˜¯ä¿å®ˆå‹æŠ•èµ„è€…ï¼Œè¯¥é€‰ä»€ä¹ˆç†è´¢æ–¹å¼ï¼Ÿ"),
            ("customer_001", "è§£é‡Šä¸€ä¸‹ä»€ä¹ˆæ˜¯å¤åˆ©ï¼Ÿ"),
            ("customer_002", "åŒ—äº¬ä»Šå¤©çš„å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"),
            ("customer_002", "è´§å¸åŸºé‡‘å’Œå€ºåˆ¸åŸºé‡‘æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ")
        ]

        # æ‰§è¡Œæµ‹è¯•ç”¨ä¾‹
        for customer_id, query in test_cases:
            await agent.handle_user_query(customer_id, query)
            print("\n" + "-"*50)  # åˆ†éš”ç¬¦

    print("âœ… æ‰€æœ‰æŸ¥è¯¢å¤„ç†å®Œæˆï¼Œç¨‹åºé€€å‡º")

if __name__ == "__main__":
    # è§£å†³Windows/Linuxå¼‚æ­¥äº‹ä»¶å¾ªç¯å·®å¼‚
    try:
        asyncio.run(main())
    except RuntimeError as e:
        if "cannot be called from a running event loop" in str(e):
            # é€‚é…Jupyterç­‰å·²æœ‰äº‹ä»¶å¾ªç¯çš„ç¯å¢ƒ
            loop = asyncio.get_event_loop()
            loop.run_until_complete(main())
        else:
            raise
